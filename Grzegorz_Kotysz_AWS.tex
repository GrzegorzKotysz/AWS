\documentclass[11pt,english]{article}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}  % san serif font
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage{array}
\usepackage{float}
\usepackage{multirow}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage{listings} % enables source code to be pasted
\usepackage[stable]{footmisc} % enables footnotes in sections etc
\usepackage{hyperref} % enables hyperlinks


\makeatletter
\makeatother

\usepackage{babel}
\begin{document}

\thispagestyle{empty}
\title{OpenFOAM in AWS}

\author{Grzegorz Kotysz}

\date{}

\maketitle
\newpage{}

\tableofcontents{}

\newpage{}

\section{Introduction}

The purpose of this paper is to learn how to use AWS computing cloud to run OpenFOAM cases using CLI, setting up cloud computing cluster and transferring results to the local computer. Simulation case will be based on dam break tutorial case.

\section{CFD Direct AWS Tutorial\footnote{CFD Direct, URL: \url{https://cfd.direct/cloud/aws/}, Accessed: 25.10.2018}}

\subsection{Instance setup\footnote{CFD Direct, URL: \url{https://cfd.direct/cloud/aws/setup}, Accessed: 25.10.2018} and launch\footnote{CFD Direct, URL: \url{https://cfd.direct/cloud/aws/launch}, Accessed: 25.10.2018}}

Before any interaction with AWS instances, user has to create an account and then create Key Pair, which will be used to authorize connection with the instance. It is recommended that it is saved in .ssh directory. Appropriate permissions also need to be set, so that only user can read this file. It can be done via terminal by executing following command:
\begin{lstlisting}
chmod 400 path/file
\end{lstlisting}
Launching an Instance will be based on the one prepared by CFD Direct with OpenFOAM preinstalled. It can be accessed via \url{https://aws.amazon.com/marketplace/pp/B017AHYO16/}. Then click "Continue to Subscription" to be able to launch Instances. Next click "Continue to Configuration". Here user can choose version of software and select region. Next click "Continue to Launch".
Here user specifies whether to launch the Instant through EC2 or from Website. In our case it is EC2 Console. Now the preferred instance is chosen, for the sake of tutorial free t2.micro is used. Click "Review and Launch". In the new window find "Edit Security Groups" and in "Source" drop-down menu select "My IP" option. Click "Review and Launch". Similarly one can edit storage size by selecting "Edit Storage", setting desirable storage and clicking "Review and Launch".
The next step is to launch the instant by clicking "Launch" button, select appropriate Key Pair and click "Launch Instances". Instance should be initialized and user should note IPv4 Public IP address.

\subsection{Connecting to an Instance and testing OpenFOAM\footnote{CFD Direct, URL: \url{https://cfd.direct/cloud/aws/connect}, Accessed: 25.10.2018}}

Connection with the instance can be established via SSH by running following command in the terminal:
\begin{lstlisting}
ssh -i path/SSHkey instanceUsername@publicIP
\end{lstlisting}	
Note: default username is "ubuntu".
User is presented with terminal prompt. Now OpenFOAM can be tested. It will be done by changing to \$FOAM\_RUN, copying \textit{pitzDaily} case from \textit{tutorials} directory, changing to copied case directory, generating mesh and running \textit{simpleFoam} solver. This can be done by following commands:
\begin{lstlisting}
run
cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily .
cd pitzDaily
blockMesh
simpleFoam
\end{lstlisting}
Finally, data can be transferred between local machine and remote instance by scp command, i.e. copying from instance to local machine:
\begin{lstlisting}
scp -i path/SSHkey instanceUsername@publicIP:path/file localPath
\end{lstlisting}
and from local machine to remote instance:
\begin{lstlisting}
scp -i path/SSHkey localPath/file instanceUsername@publicIP:path/
\end{lstlisting}

\subsection{\label{Creating a CFD Cluster}Creating a CFD Cluster\footnote{CFD Direct, url: \url{https://cfd.direct/cloud/aws/cluster}, Accessed: 25.10.2018}}

The example presented in the tutorial uses c4.8xlarge instances, however t4.micro can be used for testing purpose if no placement group is defined. If placement group is used, one must check whether chosen instance type can be used as a part of the placement group\footnote{AWS, URL: \url{https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html}, Accessed: 08.11.2018}. Cluster will be based on 1 master instance with appropriate storage for the simulation and 3 slave instances with minimal storage.

Note: Placement Group determines how instances are placed within Amazon hardware. For best performance, instances should be placed within \textit{Cluster} group, which ensures that instances are places in low-latency group.\footnote{Amazon, URL: \url{https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html}, Accessed: 26.12.2018}

The one time configuration of security group is needed to enable access between instances. At this point it will be created from GUI.
\begin{enumerate}
	\item Select \textit{Security Groups} in AWS console.
	\item Click \textit{Create Security Group}.
	\item In the pop-up windows specify \textit{Group Name} and \textit{Description} which will specify the role for the group.
	\item Choose \textit{Add Rule} option and for \textit{Inbound} select \textbf{SSH} Type and \textbf{My IP} Source.
	\item Click \textit{Create}.
\end{enumerate}
In order to allow instances to connect with each other within cluster, one must edit created security group.
\begin{enumerate}
	\item Select desired security group and for \textit{Inbound} rules tab click \textit{Edit}.
	\item Click \textit{Add Rule} and select \textit{All TCP} Type.
	\item Select \textbf{Custom} Source and start typing security group name until a panel pops up. Then select your \textit{cluster group}.
\end{enumerate}
To ensure smooth and fast operation of a cluster, instances need to be connected through a low latency, high speed network. It can be achieved by running them in appropriate placement group.
\begin{enumerate}
	\item In AWS console select \textit{Placement Groups}.
	\item Choose \textit{Create Placement Group}.
	\item Enter group name, select \textbf{cluster} in \textit{Strategy} field and click \textit{Create}.
\end{enumerate}
The next step is to launch instances. In the tutorial instances are created one at a time, but placement group documentation suggests to create all the cluster's instances at once to ensure that there is enough capacity on a given server. The procedure is as follows:
\begin{enumerate}
	\item Choose an instance that supports placement groups.
	\item Under \textit{Instance Details} select \textit{group\_name} from the \textit{Placement Group} menu.
	\item Under \textit{Security Group} choose \textit{Select and existing security group} and choose appropriate group.
	\item Under \textit{Storage} set a volume size which will be enough to store the data.
	\item Click \textit{Launch}.
\end{enumerate}
Slave instances can be created all at once by specifying number of instances under \textit{Instance Details}. For the slave instances disable \textit{Auto-assign Public IP} under \textit{Instance Details} as these instances only need to communicate with the master.

\subsubsection{Connecting instances}
Once per login session user must enable SSH access from master instance to the slave instance by using \textit{SSH agent forwarding}. It allows using to use agent forwarding instead of storing key on master node, which is not advised. Agent forwarding is enabled by adding private key to the authentication agent by the following command:
\begin{lstlisting}
ssh-add path/SSHkey
\end{lstlisting}
Now user can SSH login without specifying key by using -A option:
\begin{lstlisting}
ssh -A ubuntu@master_ip
\end{lstlisting}
Cluster is set up in a way that all data is stored on a volume attached to the master instance. Hence, OpenFOAM directory on master instance must be shared across slave instances using network file system (NFS) protocol. Firstly OpenFOAM directory is exported with \textit{exportfs} (one time only) and then mounted by all slave instances using \textit{mount}.
OpenFOAM directory can be exported from master instance terminal by adding OpenFOAM directory to the /etc/exports file as superuser, exporting this file and starting the NFS server:
\begin{lstlisting}
sudo sh -c "echo '/home/ubuntu/OpenFOAM *(rw,sync,no_subtree_check)' >> /etc/exports"
sudo exportfs -ra
sudo service nfs-kernel-server start
\end{lstlisting}
The next step is to delete empty directories in the OpenFOAM directory on each slave instance and use that directory as a mount point for mounting the OpenFOAM directory on the master instance. For this user needs private IP of the master instance and private IP of each slave instance. This can be partially automated by defining environment variable which will store private IPs of slave instances. It has to be done each time instance is started.
\begin{lstlisting}
VAR="ip_slave1 ip_slave2 ip_slave3 ..."
for IP in $VAR ; do ssh $IP 'rm -rf ${HOME}/OpenFOAM/*' ; done
for IP in $VAR ; do ssh $IP 'sudo mount ip_master:${HOME}/OpenFOAM ${HOME}/OpenFOAM' ; done
\end{lstlisting}
Mounting of the OpenFOAM directory can be tested for each slave by the command:
\begin{lstlisting}
for IP in $VAR ; do ssh $IP 'ls ${HOME}/OpenFOAM' ; done
\end{lstlisting}
It should return \textit{ubuntu-<version>} directory.

\subsubsection{Running test case}
Cluster will be tested on the damBreak tutorial case. It will involve changing to \$FOAM\_RUN directory, copying case from the \textit{tutorials} directory, generating a mesh, refining the mesh, creating field file and initialising with \textit{setFields}. Then mesh will be decomposed using \textit{decomposePar} utility and running \textit{interFoam} solver in parallel.
First log onto the master node:
\begin{lstlisting}
ssh -A ubuntu@master_ip
\end{lstlisting}
Then prepare the simulation by executing following commands:
\begin{lstlisting}
run
cp -r $FOAM_TUTORIALS/multiphase/interFoam/laminar/damBreak/damBreak .
cd damBreak
blockMesh
refineMesh -overwrite
cp -r 0/alpha.water.orig 0/alpha.water
setFields
decomposePar
\end{lstlisting}
Note: number of subdomains is specified in \textit{decomposeParDict} file in \textit{system} directory.
User needs to specify a list of host machines to run case parallel on a cluster. It should be specified in \textit{machines} file in case directory. File content should look like this:
\begin{lstlisting}
ip_host1
ip_host2
ip_host3
.
.
.
\end{lstlisting}
Case can be run in parallel on a number of cores specified by \textit{processor} directories using the host names in the \textit{machines} file using \textit{foamJob} script with -p flag:
\begin{lstlisting}
foamJob -p interFoam
\end{lstlisting}

NOTE: author's tests showed that it is  better to disable hyperthreading during creation of Instances, because it gives no improvement in calculation time.

\section{AWS CLI}
Note: this section is based on AWS CLI (Command Line Interface) documentation which can be found at AWS website.
\subsection{\label{Install AWS CLI}Installing AWS CLI on Linux}
According to the AWS CLI documentation the best way to assure that the newest version is installed, user should install AWS CLI via \textit{pip}. The process of installing \textit{pip} if it is not present on the system will not be described as it can be found easily on the web.
AWS CLI can be installed by the command:
\begin{lstlisting}	
pip install awscli --upgrade --user
\end{lstlisting}
\textit{--upgrade} updates all requirements and \textit{--user} option installs the program to a subdirectory of user directory.

\subsection{Configuring AWS CLI}
The fastest way to start is to execute \textit{aws configure} command. This will add default profile, which will be used for executing AWS command if no profile will be explicitly specified. User must specify AWS Access Key ID, AWS Secret Access Key, default region name and default output format. \textit{AWS Access Key} and \textit{AWS Secret Access Key} are connected with AWS credentials connected with IAM user or role. \textit{Access Key ID} and \textit{Secret Access Key} can be created via IAM console in \textit{Security credentials} tab. \textit{Default region name} specifies appropriate server. \textit{Default output format} specifies format of the results (options include: json, text, table).

Other profiles can be created by executing \textit{aws configure} with \textit{--profile} flag:
\begin{lstlisting}
aws configure --profile user2
\end{lstlisting}
Then, commands can be run via default profile, or specified by aforementioned flag. I.e.
\begin{lstlisting}
aws s3 ls
aws s3 ls --profile myuser
\end{lstlisting}

Credentials data can be stored in many places on the computer and the precedence is defined in the following order:
\begin{enumerate}
	\item Command line options, i.e. \textit{--region, --output, --profile} flags.
	\item Environment variables: \textit{AWS\_ACCESS\_KEY\_ID, AWS\_SECRET\_ACCESS\_KEY, AWS\_SESSION\_TOKEN}.
	\item CLI credentials file located at \textit{~/.aws/credentials}.
	\item CLI configuration file located at \textit{~/.aws/config}.
	\item Container credentials.
	\item Instance profile credentials.
\end{enumerate}

\subsubsection*{Using environment variables}
Non-default profile can be used for single command as described above, but for multiple commands it can be tedious. Hence, one can create temporary environment variable (lasting until the end of shell session) by executing following command:
\begin{lstlisting}
export AWS_PROFILE=profile_name
\end{lstlisting}
Additionally, another environment variables can be set to change options for shell process. List of available commands can be found in \href{https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html}{AWS Documentation, section Environment Variables}.

\subsubsection*{Command Completion}
AWS CLI includes command-completion feature which can be used with the \textbf{TAB} key. It might not be installed automatically. 
First, start by locating shell's installation directory by running:
\begin{lstlisting}
echo $SHELL
\end{lstlisting}
Then, locate AWS Completer by running:
\begin{lstlisting}
which aws_completer
\end{lstlisting}
Depending on shell type, different command must be used to enable AWS Completer. In case of bash, add
\begin{lstlisting}
complete -C 'path_to_aws_completer/aws_completer' aws
\end{lstlisting}
into \textit{~/.bashrc} file if you want it to be enabled every time you launch the shell.
 	
\section{Running EC2 Instance via AWS CLI\footnote{AWS, URL: \url{https://docs.aws.amazon.com/cli/latest/userguide/tutorial-ec2-ubuntu.html}, Accessed: 12.12.2018}}

First of all, install AWS CLI as described in section \ref{Install AWS CLI}. Then, run \textit{aws configure} and set up your credentials.
Verify that credentials are configured correctly by executing:
\begin{lstlisting}
aws ec2 describe-regions --output table
\end{lstlisting}
It should return table containing list of regions available at AWS.
The next step is to create a Security Group and Key Pair. As mentioned in section \ref{Creating a CFD Cluster}, rule must be added to allow connection via SSH tunnel. Note that \textit{--vpc-id} parameter will be omitted as default VPC will be used. Run
\begin{lstlisting}
aws ec2 create-security-group --group-name tutorial-sg --description "security group for tutorial"
\end{lstlisting}
It should return security group ID, which will be needed when launching the instance. Set described permission with following command:
\begin{lstlisting}
aws ec2 authorize-security-group-ingress --group-name tutorial-sg --protocol tcp --port 22 --cidr 0.0.0.0/0
\end{lstlisting}
Next, create key pair to be able to connect to the instance:
\begin{lstlisting}
aws ec2 create-key-pair --key-name tutorial-key --query 'KeyMaterial' --output text > tutorial-key.pem
\end{lstlisting}
Do not forget about changing permissions for the key file:
\begin{lstlisting}
chmod 400 key_name.pem
\end{lstlisting}
Now, you are ready to launch and connect to the instance. You will need --image-id parameter which specifies AMI used. In case of OpenFOAM 6 on Ubuntu 18.04 provided by CFD Direct it is ami-e0d2d10b. Other parameters are self-explanatory. This command returns instance ID.
\begin{lstlisting}
aws ec2 run-instances --image-id ami-e0d2d10b --security-group-ids <security-group-id> --count <instances_number> --instance-type <i.e. t2.micro> --key-name tutorial-key --query 'Instances[0].InstanceId'
\end{lstlisting}
When instance is running, you can query public ip address, so you can connect to this instance.
\begin{lstlisting}
aws ec2 describe-instances --instance-ids <instance ID> --query 'Reservations[0].Instances[0].PublicIpAddress'
\end{lstlisting}
Now you can connect via ssh to your Instance.

\subsection{CLI Skeleton\footnote{AWS, URL: \url{https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-skeleton.html}, Accessed: 14.12.2018}}

In order to speed up creation of instances, CLI Skeleton can be prepared. CLI Skeleton in JSON file which stores parameters of \textit{aws ec2 run-instance} command. User can specify any combination of parameters within file. This file template can be generated by running command (and saved to file):
\begin{lstlisting}
aws ec2 run-instances --generate-cli-skeleton > file
\end{lstlisting}
After creating this template, it can be tailored to user's needs by editing in any text editor. Note that when for example KeyName is specified, user does not specify extension of file, rather just name of key. As most often the same set of parameters will be specified during creation of instances, CLI Skeleton will be used for creating OpenFOAM cluster via CLI. Useful parameter for template testing purposes is \textit{DryRun} as when set to \textit{true} it checks whether JSON is formed correctly.

\section{Script for cluster creation}

The attached script will create appropriate IAM role, instance profile, placement group, security group, then run specified instances, copy data to these instances, run selected script with OpenFOAM instructions, send data to S3 bucket and finally, terminate all instances connected with the case.

First, variables containing names and paths are specified:
\begin{lstlisting}
caseName="test"

instanceType="c5.large" #c5.large
numberOfCPUs=1
numberOfNodes=2 # how many nodes (including master)

diskDeviceName="/dev/sda1" # edit disk space
diskSize=20 # in GB

openFoamScript=~/Documents/AWS/Scripts/OFAWS.sh
masterCliSkeleton=~/Documents/AWS/Scripts/templateMaster.json
slaveCliSkeleton=~/Documents/AWS/Scripts/templateSlave.json
tempDir=~/Documents/AWS/temp/

S3Bucket=""
userName="ubuntu" # as in host

placementGroupName="ClusterTest"
securityGroupName="ClusterTest"
securityGroupDescription="Security Group created by runCluster script"

EC2trustFile=~/Documents/AWS/Scripts/iamEC2trust.json
permissionsPolicyFile=~/Documents/AWS/Scripts/iamPermissionsPolicy.json

sshKey=~/.ssh/tutorial-key.pem
sshKeyName="tutorial-key"
region=eu-central-1
tag=Case # tag name for iam policies
\end{lstlisting}
The following list explains meaning of non self-explanatory variables:
\begin{itemize}
	\item \textit{caseName} determines name of the folder where data will be stored as well as tag for instances and names for IAM policies, roles and instance profile,
	\item \textit{instanceType} is self-explanatory, however, user must bear in mind that not all instance types allow for certain options used in .json templates, especially specifying CPU options,
	\item \textit{numberOfCPUs} can be specified if instance type allows, so user can use less physical processors than default for instance type,
	\item \textit{numberOfNodes} means how many instances will be created,
	\item \textit{openFoamScript} specifies path to the script containing OpenFOAM connected commands which shall be executed on master node,
	\item \textit{masterCliSkeleton, slaveCliSkeleton} specifies path to templates for starting instances,
	\item \textit{S3Bucket} stores path to S3 folder, i.e. xxx/xyz/, where xxx is bucket name and xyz is folder within bucket,
	\item \textit{EC2trustFile} stores path to file containing JSON data about policy allowing EC2 instance to assume role,
	\item \textit{permissionsPolicyFile} stores path to JSON file with IAM policy specification.
\end{itemize}
Then, derived variables are created containing information about instances names, IAM names and about temporary files.
\begin{lstlisting}
masterName="\$caseName""Master"
slaveName="\$caseName""Slave"
iamPolicyName="\$caseName""Policy"
iamRoleName="\$caseName""Role"
instanceProfileName="\$caseName""Profile"
S3BucketFolder="\$S3Bucket""\$caseName"

(( numberOfSlaves= \$numberOfNodes - 1 ))
tempMasterCliSkeleton="\$tempDir""tempMaster.json"
tempSlaveCliSkeleton="\$tempDir""tempSlave.json"
tempPermissionsPolicyFile="\$tempDir""tempIamPermissionsPolicy.json"
remoteOFScript="\textbackslash\{\}\$FOAM\_RUN/OFaws.sh"
\end{lstlisting}
Next, specified files are checked, i.e. whether they exist and are readable and if instance type is correct.
\begin{lstlisting}
if [ ! -r $sshKey ] ; then
printf "sshKey does not exist: %s\n" "$sshKey"
exit 1
fi
if [ ! -r $masterCliSkeleton ] ; then
printf "Master CLI Skeleton does not exist: %s\n" "$masterCliSkeleton"
exit 1
fi
if [ ! -r $slaveCliSkeleton ] ; then
printf "Slave CLI Skeleton does not exist: %s\n" "$slaveCliSkeleton"
exit 1
fi
if [ ! -r $openFoamScript ] ; then
printf "OpenFOAM script does not exist: %s\n" "$openFoamScript"
exit 1
fi
if [[ $(aws pricing get-attribute-values --service-code AmazonEC2 --attribute-name instanceType --region us-east-1 --output text | sed 's/ATTRIBUTEVALUES\t//g' | grep -w $instanceType -c ) -eq 0 ]] ; then
printf "ERROR! Invalid instance type:""$instanceType""\n"
exit 1
fi
\end{lstlisting}
Now, placement group is created if it does not already exist:
\begin{lstlisting}
if [[ $(aws ec2 describe-placement-groups --output text | sed 's/PLACEMENTGROUPS\t//' | sed 's/\t.*//' | grep -w -i $placementGroupName -c) -eq 0 ]] ; then
aws ec2 create-placement-group --group-name $placementGroupName --strategy cluster
fi
\end{lstlisting}
Then, security group is created if it does not exist. It is done by creating \textit{myip} variable first, so inbound connection can only be done from this ip. Also, the connection within security group must be allowed, so the communication between master and slaves is possible.
\begin{lstlisting}
myip=$(curl http://checkip.amazonaws.com)
if [[ $(aws ec2 describe-security-groups --query 'SecurityGroups[*].GroupName' --output text | grep -w "$securityGroupName" -c) -eq 0 ]] ; then
	aws ec2 create-security-group --group-name "$securityGroupName" --description "$securityGroupDescription"
	aws ec2 authorize-security-group-ingress --group-name "$securityGroupName" --protocol tcp --port 0-65535 --source-group "$securityGroupName"
fi

aws ec2 authorize-security-group-ingress --group-name "$securityGroupName" --protocol tcp --port 22 --cidr ${myip}/32
securityGroupId=$(aws ec2 describe-security-groups --query 'SecurityGroups[*].[GroupName, GroupId]' --output text | grep "$securityGroupName" | sed 's/.*\t//')
\end{lstlisting}
Next, temporary files are edited to reflect user specified parameters:
\begin{lstlisting}
# create temporary templates
mkdir $tempDir
cp $masterCliSkeleton $tempMasterCliSkeleton
cp $slaveCliSkeleton $tempSlaveCliSkeleton

# edit temporary template so appropriate placement group is used
sed -i 's/"GroupName": "",/"GroupName": "'"$placementGroupName"'",/' $tempMasterCliSkeleton $tempSlaveCliSkeleton

# edit security group ID
sed -i 's/"tutorial-sg"/"'"$securityGroupId"'"/' $tempMasterCliSkeleton $tempSlaveCliSkeleton

# edit instance type
sed -i 's/"InstanceType": "c5.large",/"InstanceType": "'"$instanceType"'",/' $tempMasterCliSkeleton $tempSlaveCliSkeleton

# edit instances number
sed -i -e 's/"MaxCount": ,/"MaxCount": '$numberOfSlaves',/' -e 's/"MinCount": ,/"MinCount": '$numberOfSlaves',/' $tempSlaveCliSkeleton

# edit cpu core number
sed -i 's/"CoreCount": ,/"CoreCount": '$numberOfCPUs',/' $tempMasterCliSkeleton $tempSlaveCliSkeleton

# edit names
sed -i 's/"Value": "defaultName"/"Value": "'"$masterName"'"/' $tempMasterCliSkeleton
sed -i 's/"Value": "defaultName"/"Value": "'"$slaveName"'"/' $tempSlaveCliSkeleton

# add disk space
sed -i 's\"DeviceName": ""\"DeviceName": "'"$diskDeviceName"'"\' $tempMasterCliSkeleton
sed -i 's/"VolumeSize": 0/"VolumeSize": '$diskSize'/' $tempMasterCliSkeleton

# edit key [case name]
sed -i -e 's/tag-name/'$tag'/' -e 's/tag-value/'$caseName'/' $tempMasterCliSkeleton $tempSlaveCliSkeleton
\end{lstlisting}
Important information for IAM policies is account ID. In this script it is acquired by aws sts get-caller-identity command.
\begin{lstlisting}
userID=$(aws sts get-caller-identity --query 'Account' --output text)
\end{lstlisting}
In the following section, instance profile will be created, however, there are some steps needed to be taken before to ensure no errors. First of all, if instance profile with given name exists, it must be deleted. In order to achieve this, all roles have to be removed from instance profile. Then, it can be deleted. The next step is to remove old IAM role, which has the same name as the one user wants to create. IAM role cannot be deleted if it has policies attached, so first policies are detached and then role is deleted. 
\begin{lstlisting}
if [ $(aws iam list-instance-profiles --query InstanceProfiles[*].InstanceProfileName | grep -w $instanceProfileName -c) -eq 1 ] ; then
if ! [ "$(aws iam get-instance-profile --instance-profile-name $instanceProfileName --query "InstanceProfile".Roles[*].RoleName)" == 'null' ] ; then
aws iam remove-role-from-instance-profile --instance-profile-name $instanceProfileName --role-name $(aws iam get-instance-profile --instance-profile-name $instanceProfileName --query "InstanceProfile".Roles[*].RoleName --output text)
fi
aws iam delete-instance-profile --instance-profile-name $instanceProfileName
fi

if [ $(aws iam list-roles --query Roles[*].RoleName | grep -w $iamRoleName -c) -eq 1 ] ; then
for policyAttached in $(aws iam list-role-policies --role-name $iamRoleName --query PolicyNames[*] --output text)
do
aws iam delete-role-policy --role-name $iamRoleName --policy-name $policyAttached
done
aws iam delete-role --role-name $iamRoleName
fi
\end{lstlisting}
Now, new role can be created with policy specifying that EC2 service instances can assume role. Then, temporary file is created, so that S3 bucket etc. can be specified prior to creating new instances. Next, policy is attached to the role, which enables sending data to S3 bucket and terminating instances with given resource tag. Finally, role is added to instance profile and instance profile is associated with master instance CLI-skeleton. Due to the time needed for AWS to coordinate data across their services, sleep command was added as without this wait, instance creation signal was sent before instance profile was ready, which lead to and error and lack of creation of master instance. Finally, instances are created and temporary files are removed.
\begin{lstlisting}
cp $permissionsPolicyFile $tempPermissionsPolicyFile

sed -i -e 's|defaultBucket|'"$S3BucketFolder"'/*|' \
-e 's|arn:aws:ec2:region:userID:instance|arn:aws:ec2:'$region':'$userID':instance|' \
-e 's|"ec2:ResourceTag/tag-name":"tag-value"|"ec2:ResourceTag/'$tag'":"'$caseName'"|' "$tempPermissionsPolicyFile"

aws iam put-role-policy --role-name $iamRoleName --policy-name $iamPolicyName --policy-document file://$tempPermissionsPolicyFile

if [ $(aws iam list-instance-profiles --query InstanceProfiles[*].InstanceProfileName | grep -w testProfile -c) -eq 0 ] ; then
aws iam create-instance-profile --instance-profile-name $instanceProfileName
fi

aws iam add-role-to-instance-profile --instance-profile-name $instanceProfileName --role-name $iamRoleName

sed -i 's|instanceProfileName|'$instanceProfileName'|' $tempMasterCliSkeleton

sleep 15s

aws ec2 run-instances --cli-input-json file://"$tempMasterCliSkeleton"
aws ec2 run-instances --cli-input-json file://"$tempSlaveCliSkeleton"

rm -r $tempDir
\end{lstlisting}
Agent forwarding shall be enabled so the user can communicate with slave instances via master.
\begin{lstlisting}
ssh-add $sshKey
\end{lstlisting}
Then, one should wait for initialization of instances to be finished.
\begin{lstlisting}
while [[ ! ( $(aws ec2 describe-instances --filter Name=tag:Name,Values=$masterName --query 'Reservations[*].Instances[*].[State.Name]' --output text | grep -w "running" -c) -eq 1 && $(aws ec2 describe-instances --filter Name=tag:Name,Values=$slaveName --query 'Reservations[*].Instances[*].[State.Name]' --output text | grep -w "running" -c) -eq $numberOfSlaves ) ]] 
do
printf "Waiting for initialization\n"
sleep 3s
done
printf "Initialization finished \n"
\end{lstlisting}
Then, public ip of master node must be gathered in order to be able to connect via ssh. In addition, private ip numbers of master node and all slaves are downloaded. These are needed for NFS server connection and for mpi library later used in OpenFOAM parallel calculations.
\begin{lstlisting}
masterIP="NULL"
while [[ $masterIP == "NULL" ]]
do
masterIP=$(aws ec2 describe-instances --filter Name="tag:Name",Values=$masterName Name="instance-state-name",Values="running" --query 'Reservations[*].Instances[*].[PublicIpAddress]' --output text)
done

masterPrivateIP=$(aws ec2 describe-instances --filter Name="tag:Name",Values=$masterName Name="instance-state-name",Values="running" --query 'Reservations[*].Instances[*].[PrivateIpAddress]' --output text)

slavesPrivateIPs=$(aws ec2 describe-instances --filter Name="tag:Name",Values=$slaveName Name="instance-state-name",Values="running" --query 'Reservations[*].Instances[*].[PrivateIpAddress]' --output text | tr '\n' ' ') # format ip1 ip2 ip3
\end{lstlisting}
In the next step, script containing OpenFOAM commands is copied, however, if simulation case is not a variation of tutorial case, i.e. it can be copied from tutorial with minor modification, user should also copy necessary files at this point. The purpose of this loop is to ensure files are really copied, as it happened during testing that even though the state of instances was "running", ssh connection failed. Hence the double checking.
\begin{lstlisting}
while 
scp -q "$openFoamScript" "$userName""@""$masterIP":"$remoteOFScript"
ssh -q "$userName""@""$masterIP" ! test -e "$remoteOFScript"
do 
printf "OF script yet to be copied, please wait\n"
sleep 3s
done
if ssh -q "$userName""@""$masterIP" test -e "$remoteOFScript" ; then
printf "OF script copied successfully\n"
else   
printf "ERROR encountered while copying OF script, exiting...\n"
exit 1
fi
\end{lstlisting}
The next step is to create NFS server, so all data can be store at master node, which enables slave disk to be small and hence saving some credits. One of the steps is to create known host file at master node, so all the ssh connection can be done in background without prompting user for input. All of the following ssh commands could be sent simultaneously.
\begin{lstlisting}
ssh -A "$userName""@""$masterIP" /bin/bash << EOF
sudo sh -c "echo '/home/'$userName'/OpenFOAM *(rw,sync,no_subtree_check)' >> /etc/exports"
sudo exportfs -ra
sudo service nfs-kernel-server start
EOF

ssh -A "$userName""@""$masterIP" /bin/bash << EOF
ssh-keyscan -H -t rsa $slavesPrivateIPs  >> ~/.ssh/known_hosts
EOF

ssh -A "$userName""@""$masterIP" /bin/bash << EOF
for ip in $slavesPrivateIPs ; do 
ssh \$ip 'rm -rf \${HOME}/OpenFOAM/*' 
done
EOF

ssh -A "$userName""@""$masterIP" /bin/bash << EOF
for ip in $slavesPrivateIPs ; do
ssh \$ip "sudo mount $masterPrivateIP:\${HOME}/OpenFOAM \${HOME}/OpenFOAM"
done
EOF

ssh -A "$userName""@""$masterIP" /bin/bash << EOF
for ip in $slavesPrivateIPs ; do
ssh \$ip 'ls \${HOME}/OpenFOAM' ;
done
EOF
\end{lstlisting}
Next step is to create list of private ip numbers for mpi library.
\begin{lstlisting}
listOfIPs='/home/ubuntu/OpenFOAM/ipList.txt'

ssh -A "$userName""@""$masterIP" /bin/bash << EOF
printf "$masterPrivateIP"" $slavesPrivateIPs" | tr ' ' '\n' > $listOfIPs
\end{lstlisting}
One last piece of information must be fetched from AWS, namely IDs of all instances with given tag, so all nodes will be terminated after end of calculations.
\begin{lstlisting}
instancesIDs=$(aws ec2 describe-instances --filter Name=tag:$tag,Values=$caseName Name="instance-state-name",Values="running" --query 'Reservations[*].Instances[*].[InstanceId]' --output text | tr '\n' ' ')
\end{lstlisting}
Finally, the set of commands are sent to master node with nohup command, so these commands will be executed independently from user's machine state. It launches OF script, waits for it to finish, then sends data to S3 bucket and terminates all instances.
\begin{lstlisting}
nohup ssh -A "$userName""@""$masterIP" /bin/bash << EOF &
source "$remoteOFScript" $numberOfNodes $listOfIPs &
OFPID=\$(echo \$!)
wait $OFPID
aws s3 cp --recursive "\$FOAM_RUN" s3://"$S3BucketFolder"
aws ec2 terminate-instances --region eu-central-1 --instance-ids $instancesIDs
EOF
\end{lstlisting}
Meanwhile at user's computer, command is sent to AWS to revoke security group rule allowing connection via ssh to master node, making cluster unreachable and safe. If user needs to connect later to master node, i.e. to check if calculations are being done, new inbound rule must be created as it was described already.
\begin{lstlisting}
aws ec2 revoke-security-group-ingress --group-name "$securityGroupName" --protocol tcp --port 22 --cidr ${myip}/32
\end{lstlisting}

\end{document}

